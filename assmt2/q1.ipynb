{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451fafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26e51468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start defining a pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a source - left grayscale cameras\n",
    "cam_left = pipeline.createMonoCamera()\n",
    "cam_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "cam_left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_480_P)\n",
    "\n",
    "# Create outputs\n",
    "xout_left = pipeline.createXLinkOut()\n",
    "xout_left.setStreamName('left')\n",
    "cam_left.out.link(xout_left.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44a12dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RECORDING FOR 10 SECONDS\n",
    "## THE FRAMES ARE BEING STORED IN DIRECTORY \"video_frames\"\n",
    "\n",
    "# Connect and start the pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "\n",
    "    # Output queue will be used to get the grayscale frames from the output defined above\n",
    "    q = device.getOutputQueue(name=\"left\", maxSize=4, blocking=False)\n",
    "\n",
    "    # Make sure the destination path is present before starting to store the examples\n",
    "    Path(f\"video_frames/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # running loop for 10 secs\n",
    "    ten_secs = time.time() + 10\n",
    "    \n",
    "    while time.time() < ten_secs:\n",
    "        # Blocking call, will wait until a new data has arrived\n",
    "        inSrc = q.get()  \n",
    "        # Data is originally represented as a flat 1D array, it needs to be converted into HxW form\n",
    "        frame = inSrc.getCvFrame()\n",
    "        # Frame is transformed and ready to be shown\n",
    "        cv2.imshow(\"left\", frame)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "        cv2.imwrite(f\"video_frames/{int(time.time() * 10000)}.png\", frame)\n",
    "\n",
    "    cv2.destroyAllWindows()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712df24f",
   "metadata": {},
   "source": [
    "## Harris corner detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4380de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a random image from the captured frames\n",
    "image = cv2.imread('video_frames/16679490745743.png')\n",
    "\n",
    "# display input image\n",
    "cv2.imshow('Image input', image)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()            \n",
    "\n",
    "# converting to gray-scale\n",
    "bw_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "bw_image = np.float32(bw_image)\n",
    "\n",
    "# applying harris corner detection algorithm\n",
    "dest = cv2.cornerHarris(bw_image, 2, 5, 0.07)\n",
    " \n",
    "# Results are marked through the dilated corners\n",
    "dest = cv2.dilate(dest, None)\n",
    " \n",
    "# Reverting back to the original image,\n",
    "# with optimal threshold value\n",
    "image[dest > 0.01 * dest.max()]=[0, 0, 255]\n",
    " \n",
    "# Image with corners marked \n",
    "# The image is stored in ______ for future references\n",
    "cv2.imshow('Image with Borders', image)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c55db",
   "metadata": {},
   "source": [
    "## Canny edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "190ce3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a random image from the captured frames\n",
    "image = cv2.imread('video_frames/16679490745743.png')\n",
    "\n",
    "# display input image\n",
    "cv2.imshow('Image original', image)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# applying canny edge detection algorithm\n",
    "edges_img = cv2.Canny(image,100,200)\n",
    "\n",
    "# Image with edges marked \n",
    "# The image is stored in ______ for future references\n",
    "cv2.imshow('Image with Borders', edges_img)\n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586384af",
   "metadata": {},
   "source": [
    "## Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1aed7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('video_frames/16679490734072.png')\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "img2 = cv2.imread('video_frames/16679490775732.png')\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6d047f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create() \n",
    "\n",
    "kp_img1, desc_img1 = sift.detectAndCompute(img1, None) \n",
    "kp_img2, desc_img2 = sift.detectAndCompute(img2, None) \n",
    "\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(desc_img1, desc_img2, k=2)\n",
    "\n",
    "good_points=[]     \n",
    "for m, n in matches: \n",
    "    if(m.distance < 0.6*n.distance): \n",
    "        good_points.append(m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff33bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pts = np.float32([kp_img1[m.queryIdx] \n",
    "                .pt for m in good_points]).reshape(-1, 1, 2) \n",
    " \n",
    "train_pts = np.float32([kp_img2[m.trainIdx] \n",
    "                .pt for m in good_points]).reshape(-1, 1, 2) \n",
    " \n",
    "matrix, mask = cv2.findHomography(query_pts, train_pts, cv2.RANSAC, 5.0) \n",
    " \n",
    "matches_mask = mask.ravel().tolist() \n",
    "\n",
    "h,w = img1.shape\n",
    " \n",
    "pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    " \n",
    "dst = cv2.perspectiveTransform(pts, matrix)\n",
    " \n",
    "homography = cv2.polylines(img2, [np.int32(dst)], True, (255, 0, 0), 3) \n",
    " \n",
    "cv2.imshow(\"Homography\", homography) \n",
    "cv2.imshow(\"Img\", img1) \n",
    "cv2.waitKey(1000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5cc28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = cv2.drawMatches(img1, kp_img1, img2, kp_img2, good_points, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "cv2.imshow(\"lines\", img3)\n",
    "cv2.waitKey(10000)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54235ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
